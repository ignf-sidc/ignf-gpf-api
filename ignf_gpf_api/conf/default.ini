[logging]
# Niveaux de log expliqués ici : https://docs.python.org/2/library/logging.html#logging-levels
# 10 DEBUG 20 INFO 30 WARNING 40 ERROR 50 CRITICAL
log_level=20


[store_authentification]
############################### Paramétrage de l'authentification via KeyCloak ###############################
token_url=https://qlf-iam-gpf.ign.fr/auth/realms/master/protocol/openid-connect/token
login=LOGIN_TO_MODIFY
password=PASSWORD_TO_MODIFY
client_id=guichet
# En cas d'échec lors de l'authentification : max nb_attempts tentatives, sec_between_attempt secondes entre chacune d'entre elles
nb_attempts=5
sec_between_attempt=1


[store_api]
############################### Paramètres de l'API Entrepôt ###############################
root_url=https://api-qualification.ccs-ign-plage.ccs.cegedim.cloud/api/v1
datastore=DATASTORE_ID_TO_MODIFY
root_datastore=${store_api:root_url}/datastores/${store_api:datastore}
# En cas d'échec lors du requêtage : max nb_attempts tentatives, sec_between_attempt secondes entre chacune d'entre elles
nb_attempts=5
sec_between_attempt=1


[routing]
############################### Routes de l'API Entrepôt ###############################
# Upload
upload_list=${store_api:root_datastore}/uploads
upload_create=${routing:upload_list}
upload_get=${routing:upload_list}/{upload}
upload_delete=${routing:upload_list}/{upload}
upload_creation=${store_api:root_datastore}/uploads
upload_add_tags=${upload_get}/tags
upload_push_data=${upload_get}/data
upload_push_metadata=${upload_get}/metadata
upload_push_attachment=${upload_get}/annexes
upload_push_md5=${upload_get}/md5
upload_close=${upload_get}/close
# road_uploads_list_property_suffix={property_name}={property_value}
# road_uploads_list_tag_suffix=tags[]={tag_name}={tag_value}

# StoredDatas
stored_data_list=${store_api:root_datastore}/stored_data
stored_data_get=${stored_data_list}/{stored_data}
; road_stored_datas_list_property_suffix={property_name}={property_value}
; road_stored_datas_list_tag_suffix=tags[]={tag_name}={tag_value}

# Checks
road_available_checks=${store_api:root_datastore}/checks
road_check_log=${road_available_checks}/executions/{execution}/logs

# Processings
road_available_processings=${store_api:root_datastore}/processings
road_processing_execution_creation=${road_available_processings}/executions
road_processing_execution_read=${road_processing_execution_creation}/{execution}
road_processing_execution_launch=${road_processing_execution_read}/launch
road_processing_execution_delete=${road_processing_execution_read}
road_processing_execution_logs=${road_processing_execution_read}/logs
nb_sec_between_log_updates=10


[upload_creation]
############################### Contrainte d'unicité à la création d'une livraison ###############################
# Contrainte d'unicité définie par un ensemble de propriétés ET de tags (laisser vide si aucune). Les propriétés
# d'une même ligne sont séparées par un point-virgule
# Exemple :
# pour définir une unicité sur les attributs "name", "srs" et le tag "livraison"
# indiquez uniqueness_constraint_upload_infos=name;srs ET uniqueness_constraint_tags=livraison
uniqueness_constraint_upload_infos=name
uniqueness_constraint_tags=
# Comportement du programme bagi_creation_livraison si une livraison existe déjà (sur la base de la contrainte d'unicité)
#   - DELETE : tente de supprimer la livraison existante,
#   - CONTINUE : le programme reprend le transfert
#   - STOP : le programme affiche uniquement un message et s'arrête
behavior_if_exists=STOP

[upload_status]
open_status=OPEN
close_status=CLOSE

[miscellaneous]
# Répertoire contenant les données sur l'entrepôt
data_directory_on_store=data/
ISOAP_metadata_directory_on_store=metadata/ISOAP/
INSPIRE_metadata_directory_on_store=metadata/INSPIRE/
# Répertoire existant disposant de droits en écriture (fichiers temporaires)
tmp_workdir=/tmp
# Regex filtrant les fichiers de métadonnées à téléverser
regex_metadata_file_to_upload=^.*\.xml$


[workflow_description]
# Classe de lecture du workflow
workflow_reader=FileWorkflowReader
# Liste des paramètres génériques de bagi_preparation_donnees.
# Elle est définie de façon générique (pour l'IGN, on utilise les codes dpsg). Les paramètres sont séparés par un point-virgule.
# Les trois chaines de caractères séparées par une virgule correspondent aux trois paramètres de parser.add_argument de bagi_preparation_donnees
# Exemple : -d, --dpsg, description param dpsg;-p, --param2, description param2
generic_parameters="-d,--dpsg, Identifiant de la dpsg concernée par le traitement d'alimentation"


[workflow_resolution_regex]
# store_entity_regex permet la designation d une balise à résoudre de type storeentity
# Exemple de balise : {storeentity.upload.tags.edition[INFOS(name=toto), TAG(dpsg={param.dpsg}, type=validation)]}

# les multiplicateurs * sont suivis de ? de façon à prendre les chaînes les plus petites possibles
# Les groupes sont nommés de façon à pouvoir extraire facilement les infos. Voici les groupes :
#   - groupe entity_type : type d'entité sur l'entrepôt (upload...)
#   - groupe selected_field_type : type de champ requêté (info ou tag) sur l'entité entrepôt
#   - groupe filter_infos : filtre sur les informations d'une entité de l'entrepôt
#   - groupe filter_tags : filtre sur une entité entrepôt exploitant ses tags

# Filtre de store_entity à partir des propriétés
filter_infos = ((\s*)INFOS(\s*)\((?P<filter_infos>.*?)\)(\s*))?
# Filtre de store_entity à partir des tags
filter_tags   = ((\s*)TAGS(\s*)\((?P<filter_tags>.*?)\)(\s*))?
filter = ((\s*)\[${filter_infos},?${filter_tags}\])
store_entity_regex=(?P<all>\{(store_entity\.)(?P<entity_type>(upload|stored_data|processing_execution|offering|resource))\.(?P<selected_field_type>(tags|infos))\.(?P<selected_field>\w*)${filter}\})


[json_converter]
# Pattern de convertion des types Python en str
datetime_pattern = %Y-%m-%dT%H:%M:%S
date_pattern = %Y-%m-%d
time_pattern = %H:%M:%S


[json_schemas]
# Les chemins sont définis relativement à ignf_gpf_api/conf
descriptor_file=json_schemas/upload_descriptor_file.json
workflow_config=json_schemas/workflow_config.json
